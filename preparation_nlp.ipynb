{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка естественного языка (Natural Language Processing, NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # pip install nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Жила в старом лесу белка. У белки весной появилась дочка белочка. Один раз белка с белочкой собирали грибы на зиму. Вдруг на соседней ёлке появилась куница. Она приготовилась схватить белочку. Мама – белка прыгнула навстречу кунице и крикнула дочке: «Беги!» Белочка бросилась наутёк. Наконец она остановилась. Посмотрела по сторонам, а места незнакомые! Мамы – белки нет. Что делать? Увидела белочка дупло на сосне, спряталась и заснула. А утром мама дочку нашла.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"Жила в старом лесу белка. У белки весной появилась дочка белочка. Один раз белка с белочкой собирали грибы на зиму. Вдруг на соседней ёлке появилась куница. Она приготовилась схватить белочку. Мама – белка прыгнула навстречу кунице и крикнула дочке: «Беги!» Белочка бросилась наутёк. Наконец она остановилась. Посмотрела по сторонам, а места незнакомые! Мамы – белки нет. Что делать? Увидела белочка дупло на сосне, спряталась и заснула. А утром мама дочку нашла.\"\"\"\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделение на предложения (токенизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Жила в старом лесу белка.',\n",
       " 'У белки весной появилась дочка белочка.',\n",
       " 'Один раз белка с белочкой собирали грибы на зиму.',\n",
       " 'Вдруг на соседней ёлке появилась куница.',\n",
       " 'Она приготовилась схватить белочку.',\n",
       " 'Мама – белка прыгнула навстречу кунице и крикнула дочке: «Беги!» Белочка бросилась наутёк.',\n",
       " 'Наконец она остановилась.',\n",
       " 'Посмотрела по сторонам, а места незнакомые!',\n",
       " 'Мамы – белки нет.',\n",
       " 'Что делать?',\n",
       " 'Увидела белочка дупло на сосне, спряталась и заснула.',\n",
       " 'А утром мама дочку нашла.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# скачиваем модель, которая будет делить на предложения\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    " \n",
    "sentences = sent_tokenize(corpus, language='russian')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделение на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Жила', 'в', 'старом', 'лесу', 'белка', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(sentences[0])) # Одно предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Жила', 'в', 'старом', 'лесу', 'белка', '.', 'У', 'белки', 'весной', 'появилась', 'дочка', 'белочка', '.', 'Один', 'раз', 'белка', 'с', 'белочкой', 'собирали', 'грибы', 'на', 'зиму', '.', 'Вдруг', 'на', 'соседней', 'ёлке', 'появилась', 'куница', '.', 'Она', 'приготовилась', 'схватить', 'белочку', '.', 'Мама', '–', 'белка', 'прыгнула', 'навстречу', 'кунице', 'и', 'крикнула', 'дочке', ':', '«', 'Беги', '!', '»', 'Белочка', 'бросилась', 'наутёк', '.', 'Наконец', 'она', 'остановилась', '.', 'Посмотрела', 'по', 'сторонам', ',', 'а', 'места', 'незнакомые', '!', 'Мамы', '–', 'белки', 'нет', '.', 'Что', 'делать', '?', 'Увидела', 'белочка', 'дупло', 'на', 'сосне', ',', 'спряталась', 'и', 'заснула', '.', 'А', 'утром', 'мама', 'дочку', 'нашла', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [] # Весь текст\n",
    "\n",
    "for sentence in sentences:\n",
    "    t = word_tokenize(sentence)\n",
    "    tokens.extend(t)\n",
    " \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перевод в нижний регистр, удаление стоп-слов и знаков пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'а',\n",
       " 'без',\n",
       " 'более',\n",
       " 'больше',\n",
       " 'будет',\n",
       " 'будто',\n",
       " 'бы',\n",
       " 'был',\n",
       " 'была',\n",
       " 'были',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'в',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'во',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'все',\n",
       " 'всегда',\n",
       " 'всего',\n",
       " 'всех',\n",
       " 'всю',\n",
       " 'вы',\n",
       " 'где',\n",
       " 'да',\n",
       " 'даже',\n",
       " 'два',\n",
       " 'для',\n",
       " 'до',\n",
       " 'другой',\n",
       " 'его',\n",
       " 'ее',\n",
       " 'ей',\n",
       " 'ему',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'еще',\n",
       " 'ж',\n",
       " 'же',\n",
       " 'за',\n",
       " 'зачем',\n",
       " 'здесь',\n",
       " 'и',\n",
       " 'из',\n",
       " 'или',\n",
       " 'им',\n",
       " 'иногда',\n",
       " 'их',\n",
       " 'к',\n",
       " 'как',\n",
       " 'какая',\n",
       " 'какой',\n",
       " 'когда',\n",
       " 'конечно',\n",
       " 'кто',\n",
       " 'куда',\n",
       " 'ли',\n",
       " 'лучше',\n",
       " 'между',\n",
       " 'меня',\n",
       " 'мне',\n",
       " 'много',\n",
       " 'может',\n",
       " 'можно',\n",
       " 'мой',\n",
       " 'моя',\n",
       " 'мы',\n",
       " 'на',\n",
       " 'над',\n",
       " 'надо',\n",
       " 'наконец',\n",
       " 'нас',\n",
       " 'не',\n",
       " 'него',\n",
       " 'нее',\n",
       " 'ней',\n",
       " 'нельзя',\n",
       " 'нет',\n",
       " 'ни',\n",
       " 'нибудь',\n",
       " 'никогда',\n",
       " 'ним',\n",
       " 'них',\n",
       " 'ничего',\n",
       " 'но',\n",
       " 'ну',\n",
       " 'о',\n",
       " 'об',\n",
       " 'один',\n",
       " 'он',\n",
       " 'она',\n",
       " 'они',\n",
       " 'опять',\n",
       " 'от',\n",
       " 'перед',\n",
       " 'по',\n",
       " 'под',\n",
       " 'после',\n",
       " 'потом',\n",
       " 'потому',\n",
       " 'почти',\n",
       " 'при',\n",
       " 'про',\n",
       " 'раз',\n",
       " 'разве',\n",
       " 'с',\n",
       " 'сам',\n",
       " 'свою',\n",
       " 'себе',\n",
       " 'себя',\n",
       " 'сейчас',\n",
       " 'со',\n",
       " 'совсем',\n",
       " 'так',\n",
       " 'такой',\n",
       " 'там',\n",
       " 'тебя',\n",
       " 'тем',\n",
       " 'теперь',\n",
       " 'то',\n",
       " 'тогда',\n",
       " 'того',\n",
       " 'тоже',\n",
       " 'только',\n",
       " 'том',\n",
       " 'тот',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'ты',\n",
       " 'у',\n",
       " 'уж',\n",
       " 'уже',\n",
       " 'хорошо',\n",
       " 'хоть',\n",
       " 'чего',\n",
       " 'чем',\n",
       " 'через',\n",
       " 'что',\n",
       " 'чтоб',\n",
       " 'чтобы',\n",
       " 'чуть',\n",
       " 'эти',\n",
       " 'этого',\n",
       " 'этой',\n",
       " 'этом',\n",
       " 'этот',\n",
       " 'эту',\n",
       " 'я'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "# скачаем словарь стоп-слов\n",
    "nltk.download('stopwords')\n",
    " \n",
    "# используем set, чтобы оставить только уникальные значения\n",
    "unique_stops = set(stopwords.words('russian'))\n",
    " \n",
    "unique_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['жила', 'старом', 'лесу', 'белка', 'белки', 'весной', 'появилась', 'дочка', 'белочка', 'белка', 'белочкой', 'собирали', 'грибы', 'зиму', 'соседней', 'ёлке', 'появилась', 'куница', 'приготовилась', 'схватить', 'белочку', 'мама', 'белка', 'прыгнула', 'навстречу', 'кунице', 'крикнула', 'дочке', 'беги', 'белочка', 'бросилась', 'наутёк', 'остановилась', 'посмотрела', 'сторонам', 'места', 'незнакомые', 'мамы', 'белки', 'делать', 'увидела', 'белочка', 'дупло', 'сосне', 'спряталась', 'заснула', 'утром', 'мама', 'дочку', 'нашла']\n"
     ]
    }
   ],
   "source": [
    "no_stops = []\n",
    " \n",
    "for token in tokens:\n",
    "    token = token.lower() # перевод в нижний регистр\n",
    " \n",
    "    if token not in unique_stops and token.isalpha(): # token.isalpha() проверка, что не знак пунктуации\n",
    "        no_stops.append(token)\n",
    " \n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация\n",
    "\n",
    "Лемматизация — это процесс приведения слова к его базовой, начальной форме, или лемме.\n",
    "\n",
    "Лемматизация является важным этапом в обработке естественного языка, поскольку она помогает компьютеру «понимать» различные грамматические формы слова как одно и то же слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['жить',\n",
       " 'старый',\n",
       " 'лес',\n",
       " 'белок',\n",
       " 'белка',\n",
       " 'весна',\n",
       " 'появиться',\n",
       " 'дочка',\n",
       " 'белочка',\n",
       " 'белок',\n",
       " 'белочка',\n",
       " 'собирать',\n",
       " 'гриб',\n",
       " 'зима',\n",
       " 'соседний',\n",
       " 'ёлка',\n",
       " 'появиться',\n",
       " 'куница',\n",
       " 'приготовиться',\n",
       " 'схватить',\n",
       " 'белочка',\n",
       " 'мама',\n",
       " 'белок',\n",
       " 'прыгнуть',\n",
       " 'навстречу',\n",
       " 'куница',\n",
       " 'крикнуть',\n",
       " 'дочка',\n",
       " 'бежать',\n",
       " 'белочка',\n",
       " 'броситься',\n",
       " 'наутёк',\n",
       " 'остановиться',\n",
       " 'посмотреть',\n",
       " 'сторона',\n",
       " 'место',\n",
       " 'незнакомый',\n",
       " 'мама',\n",
       " 'белка',\n",
       " 'делать',\n",
       " 'увидеть',\n",
       " 'белочка',\n",
       " 'дупло',\n",
       " 'сосна',\n",
       " 'спрятаться',\n",
       " 'заснуть',\n",
       " 'утром',\n",
       " 'мама',\n",
       " 'дочка',\n",
       " 'найти']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "lemmas = [morph.normal_forms(word)[0] for word in no_stops]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['жить', 'старый', 'лес', 'белка', 'белка', 'весна', 'появиться', 'дочка', 'белочка', 'белка', 'белочка', 'собирать', 'гриб', 'зима', 'соседний', 'ёлка', 'появиться', 'куница', 'приготовиться', 'схватить', 'белочка', 'мама', 'белка', 'прыгнуть', 'навстречу', 'куница', 'крикнуть', 'дочка', 'бежать', 'белочка', 'броситься', 'наутёк', 'остановиться', 'посмотреть', 'сторона', 'место', 'незнакомый', 'мама', 'белка', 'делать', 'увидеть', 'белочка', 'дупло', 'сосна', 'спрятаться', 'заснуть', 'утром', 'мама', 'дочка', 'найти']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [word if word != \"белок\" else \"белка\" for word in lemmas]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стемминг\n",
    "\n",
    "Стемминг (stemming), в отличие от лемматизации, ориентирован на поиск основы слова (stem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['жит', 'стар', 'лес', 'белк', 'белк', 'весн', 'появ', 'дочк', 'белочк', 'белк', 'белочк', 'собира', 'гриб', 'зим', 'соседн', 'елк', 'появ', 'куниц', 'приготов', 'схват', 'белочк', 'мам', 'белк', 'прыгнут', 'навстреч', 'куниц', 'крикнут', 'дочк', 'бежа', 'белочк', 'брос', 'наутек', 'останов', 'посмотрет', 'сторон', 'мест', 'незнаком', 'мам', 'белк', 'дела', 'увидет', 'белочк', 'дупл', 'сосн', 'спрята', 'заснут', 'утр', 'мам', 'дочк', 'найт']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "stemmed = [stemmer.stem(s) for s in lemmas]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'белк': 5,\n",
       "         'белочк': 5,\n",
       "         'дочк': 3,\n",
       "         'мам': 3,\n",
       "         'появ': 2,\n",
       "         'куниц': 2,\n",
       "         'жит': 1,\n",
       "         'стар': 1,\n",
       "         'лес': 1,\n",
       "         'весн': 1,\n",
       "         'собира': 1,\n",
       "         'гриб': 1,\n",
       "         'зим': 1,\n",
       "         'соседн': 1,\n",
       "         'елк': 1,\n",
       "         'приготов': 1,\n",
       "         'схват': 1,\n",
       "         'прыгнут': 1,\n",
       "         'навстреч': 1,\n",
       "         'крикнут': 1,\n",
       "         'бежа': 1,\n",
       "         'брос': 1,\n",
       "         'наутек': 1,\n",
       "         'останов': 1,\n",
       "         'посмотрет': 1,\n",
       "         'сторон': 1,\n",
       "         'мест': 1,\n",
       "         'незнаком': 1,\n",
       "         'дела': 1,\n",
       "         'увидет': 1,\n",
       "         'дупл': 1,\n",
       "         'сосн': 1,\n",
       "         'спрята': 1,\n",
       "         'заснут': 1,\n",
       "         'утр': 1,\n",
       "         'найт': 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_frequency(text):\n",
    "    \"\"\"Возвращает частоту слов в словаре стем.\"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    return word_counts\n",
    "\n",
    "get_word_frequency(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ тональности текста\n",
    "\n",
    "- polarity: Полярность тональности (-1.0 - негатив, 1.0 - позитив, 0.0 - нейтральный)\n",
    "- subjectivity: Субъективность тональности (0.0 - объективный, 1.0 - субъективный)\n",
    "\n",
    "Субъективность - это когда автор выражает личное мнение, эмоции, чувства, оценки или интерпретацию событий. В субъективном тексте присутствуют слова, которые отражают индивидуальный взгляд автора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полярность: 0.0\n",
      "Субъективность: 0.0\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analysis = textblob.TextBlob(text)\n",
    "    return {\n",
    "        'polarity': analysis.sentiment.polarity,\n",
    "        'subjectivity': analysis.sentiment.subjectivity\n",
    "    }\n",
    "\n",
    "sentiment = analyze_sentiment(corpus)\n",
    "\n",
    "print(f\"Полярность: {sentiment['polarity']}\")\n",
    "print(f\"Субъективность: {sentiment['subjectivity']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст имеет нейтральную тональность.\n",
    "Текст является объективным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование данных\n",
    "\n",
    "- <span style=\"color:#CC0099\">OneHot кодирование</span> - это метод преобразования категориальных данных в бинарный вектор, где единица стоит на позиции, соответствующей этой категории, а остальные значения - нули. Главный недостаток большая размерность при наличии большого количества категорий\n",
    "- <span style=\"color:#CC0099\">Мешок слов (Bag of Words, BoW)</span> - принцип мерода в подсчете частоты встречающихся слов. **Преимущества**: простота, эффективность для обработки больших объемов текста, хорошо работает для некоторых задач (например, в задачах классификации текста, где порядок слов не так важен (классификация спама, определение темы текста)). **Недостатки**: потеря информации о порядке слов, проблема потери важной информации, если она встречается редко (иногда редкие слова не попадают в мешок).\n",
    "- <span style=\"color:#CC0099\">TF-IDF (Term Frequency-Inverse Document Frequency)</span> - Учитывает как частоту слова в тексте, так и его редкость в наборе данных. Его суть: Если слово часто встречается во всех документах (это в первую очередь касается предлогов, союзов и других стоп-слов), то вряд ли эти слова имеют большое значение. И наоборот, если слово встречаться только в одном документе, вероятно оно в большей степени определяет его содержание.\n",
    "- <span style=\"color:#CC0099\">Word2Vec</span> - Представляет слова в виде векторов, которые учитывают семантические отношения между словами.\n",
    "- <span style=\"color:#CC0099\">BERT</span> - Более продвинутый алгоритм, который учитывает контекст слова в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "lemmas_for_onehot = [[lemma] for lemma in lemmas]\n",
    "encoded_lemmas = encoder.fit_transform(lemmas_for_onehot)\n",
    "encoded_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность после OneHot кодирования: (50, 36)\n",
      "Количество слов в тексте: 50\n",
      "Количество неповторяющихся слов: 36\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размерность после OneHot кодирования: {encoded_lemmas.shape}\")\n",
    "print(f\"Количество слов в тексте: {len(lemmas)}\")\n",
    "unique_lemmas = set(lemmas)\n",
    "print(f\"Количество неповторяющихся слов: {len(unique_lemmas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словарь: ['зима', 'жить', 'старый', 'схватить', 'дупло', 'наутёк', 'незнакомый', 'белка', 'дочка', 'делать', 'броситься', 'прыгнуть', 'собирать', 'утром', 'куница', 'крикнуть', 'появиться', 'лес', 'место', 'сосна', 'навстречу', 'соседний', 'мама', 'приготовиться', 'заснуть', 'найти', 'посмотреть', 'гриб', 'бежать', 'весна', 'белочка', 'сторона', 'ёлка', 'остановиться', 'увидеть', 'спрятаться']\n",
      "Вектор 'Мешок слов': [1, 1, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1], \n",
      " его длина 36\n"
     ]
    }
   ],
   "source": [
    "def create_bow_vector(lemmas, vocabulary):\n",
    "    bow_vector = [0] * len(vocabulary)\n",
    "    for lemma in lemmas:\n",
    "        if lemma in vocabulary:\n",
    "            index = vocabulary.index(lemma)\n",
    "            bow_vector[index] += 1\n",
    "    return bow_vector\n",
    "\n",
    "vocabulary = list(set(lemmas)) # получаем уникальные леммы\n",
    "print(f\"Словарь: {vocabulary}\")\n",
    "\n",
    "bow_vector = create_bow_vector(lemmas, vocabulary)\n",
    "print(f\"Вектор 'Мешок слов': {bow_vector}, \\n его длина {len(bow_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
